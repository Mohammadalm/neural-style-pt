{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG-19 Architecture Detected\n",
      "Successfully loaded models/vgg19-d01eb7cb.pth\n",
      "conv1_1: 64 3 3 3\n",
      "conv1_2: 64 64 3 3\n",
      "conv2_1: 128 64 3 3\n",
      "conv2_2: 128 128 3 3\n",
      "conv3_1: 256 128 3 3\n",
      "conv3_2: 256 256 3 3\n",
      "conv3_3: 256 256 3 3\n",
      "conv3_4: 256 256 3 3\n",
      "conv4_1: 512 256 3 3\n",
      "conv4_2: 512 512 3 3\n",
      "conv4_3: 512 512 3 3\n",
      "conv4_4: 512 512 3 3\n",
      "conv5_1: 512 512 3 3\n",
      "conv5_2: 512 512 3 3\n",
      "conv5_3: 512 512 3 3\n",
      "conv5_4: 512 512 3 3\n",
      "Setting up style layer 2: relu1_1\n",
      "Setting up style layer 7: relu2_1\n",
      "Setting up histogram layer 7: relu2_1\n",
      "Setting up style layer 12: relu3_1\n",
      "Setting up histogram layer 12: relu3_1\n",
      "Setting up style layer 21: relu4_1\n",
      "Setting up histogram layer 21: relu4_1\n",
      "Setting up content layer 23: relu4_2\n",
      "Setting up style layer 30: relu5_1\n",
      "Setting up histogram layer 30: relu5_1\n",
      "Sequential(\n",
      "  (0): TVLoss()\n",
      "  (1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): MaskedStyleLoss(\n",
      "    (crit): MSELoss()\n",
      "    (gram): GramMatrix()\n",
      "  )\n",
      "  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (5): ReLU(inplace=True)\n",
      "  (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): ReLU(inplace=True)\n",
      "  (9): MaskedStyleLoss(\n",
      "    (crit): MSELoss()\n",
      "    (gram): GramMatrix()\n",
      "  )\n",
      "  (10): MaskedHistLoss(\n",
      "    (crit): MSELoss()\n",
      "  )\n",
      "  (11): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (12): ReLU(inplace=True)\n",
      "  (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): ReLU(inplace=True)\n",
      "  (16): MaskedStyleLoss(\n",
      "    (crit): MSELoss()\n",
      "    (gram): GramMatrix()\n",
      "  )\n",
      "  (17): MaskedHistLoss(\n",
      "    (crit): MSELoss()\n",
      "  )\n",
      "  (18): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (19): ReLU(inplace=True)\n",
      "  (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (21): ReLU(inplace=True)\n",
      "  (22): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (23): ReLU(inplace=True)\n",
      "  (24): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (25): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (26): ReLU(inplace=True)\n",
      "  (27): MaskedStyleLoss(\n",
      "    (crit): MSELoss()\n",
      "    (gram): GramMatrix()\n",
      "  )\n",
      "  (28): MaskedHistLoss(\n",
      "    (crit): MSELoss()\n",
      "  )\n",
      "  (29): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (30): ReLU(inplace=True)\n",
      "  (31): ContentLoss(\n",
      "    (crit): MSELoss()\n",
      "  )\n",
      "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (33): ReLU(inplace=True)\n",
      "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (35): ReLU(inplace=True)\n",
      "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (38): ReLU(inplace=True)\n",
      "  (39): MaskedStyleLoss(\n",
      "    (crit): MSELoss()\n",
      "    (gram): GramMatrix()\n",
      "  )\n",
      "  (40): MaskedHistLoss(\n",
      "    (crit): MSELoss()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from model import *\n",
    "from utils import *\n",
    "from generate import *\n",
    "\n",
    "# setup stylenet\n",
    "params = StylenetArgs()\n",
    "params.gpu = '0'\n",
    "params.backend = 'cudnn'\n",
    "\n",
    "dtype, multidevice, backward_device = setup_gpu(params)\n",
    "stylenet = StyleNet(params, dtype, multidevice, backward_device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f3e73c3d8ee3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# capture the style and content images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mstylenet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# initialize input image with a random image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/neural-style-pt/model.py\u001b[0m in \u001b[0;36mcapture\u001b[0;34m(self, content_image, style_images, style_blend_weights, content_masks, style_masks)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_styles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setup_style_masks__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setup_layer_masks__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__capture_content__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/neural-style-pt/model.py\u001b[0m in \u001b[0;36m__setup_style_masks__\u001b[0;34m(self, style_images)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle_masks\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             self.style_masks  = [torch.ones(style_images[i].shape).type(self.dtype) \n\u001b[0;32m--> 193\u001b[0;31m                                  for i in range(self.num_styles)]\n\u001b[0m\u001b[1;32m    194\u001b[0m         \u001b[0mstyle_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_styles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/neural-style-pt/model.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle_masks\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             self.style_masks  = [torch.ones(style_images[i].shape).type(self.dtype) \n\u001b[0;32m--> 193\u001b[0;31m                                  for i in range(self.num_styles)]\n\u001b[0m\u001b[1;32m    194\u001b[0m         \u001b[0mstyle_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_styles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# set stylenet hyper-parameters\n",
    "stylenet.set_content_weight(5e0)\n",
    "stylenet.set_style_weight(1e2)\n",
    "stylenet.set_hist_weight(0)\n",
    "stylenet.set_style_statistic('gram')\n",
    "stylenet.set_tv_weight(1e-3)\n",
    "\n",
    "# set stylenet style layers individually\n",
    "stylenet.set_style_layer(0, 'covariance', 5e3)\n",
    "stylenet.set_style_layer(1, 'gram', 1e2)\n",
    "stylenet.set_style_layer(2, 'gram', 1e2)\n",
    "stylenet.set_style_layer(3, 'gram', 1e2)\n",
    "stylenet.set_style_layer(4, 'gram', 1e2)\n",
    "# stylenet.set_hist_layer(0, 5e1)\n",
    "# stylenet.set_hist_layer(1, 5e1)\n",
    "# stylenet.set_hist_layer(2, 5e1)\n",
    "# stylenet.set_hist_layer(3, 5e1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# parameters\n",
    "image_size = 512\n",
    "style_scale = 1.0\n",
    "\n",
    "# load content image\n",
    "content_path = 'examples/inputs/monalisa.jpg'\n",
    "content_image = load_image(content_path, image_size)\n",
    "\n",
    "# load style images\n",
    "style_paths = ['examples/inputs/starry_night.jpg',\n",
    "               'examples/inputs/cubist.jpg']\n",
    "style_size = int(image_size * style_scale)\n",
    "style_images = [load_image(path, style_size) for path in style_paths]\n",
    "\n",
    "# load content masks for each style image\n",
    "content_mask_paths = ['examples/segments/monalisa1a.png',\n",
    "                      'examples/segments/monalisa1b.png']\n",
    "content_masks = [load_image(path, image_size, to_normalize=False) \n",
    "                 for path in content_mask_paths]\n",
    "\n",
    "# capture the style and content images\n",
    "stylenet.capture(content_image, style_images, None, content_masks)\n",
    "\n",
    "# initialize input image with a random image\n",
    "# or load it from another image (e.g. content)\n",
    "if True:\n",
    "    img = random_image_like(content_image)\n",
    "elif False:\n",
    "    img = load_image(content_path, image_size)\n",
    "\n",
    "# generate and save\n",
    "num_iterations = 500\n",
    "output_path = 'results/my_example.png'\n",
    "orig_colors = False\n",
    "img = optimize(stylenet, img, num_iterations, output_path, save_preview_iter=10, print_iter=250)\n",
    "if orig_colors:\n",
    "    img = original_colors(content_image, img)\n",
    "save(img, \"results/my_example.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dussde\n",
      "Running optimization with L-BFGS\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# set stylenet hyper-parameters\n",
    "stylenet.set_content_weight(5e0)\n",
    "stylenet.set_style_weight(1e2)\n",
    "stylenet.set_hist_weight(0)\n",
    "stylenet.set_style_statistic('gram')\n",
    "stylenet.set_tv_weight(1e-3)\n",
    "\n",
    "# set stylenet style layers individually\n",
    "stylenet.set_style_layer(0, 'covariance', 5e3)\n",
    "stylenet.set_style_layer(1, 'gram', 1e2)\n",
    "stylenet.set_style_layer(2, 'gram', 1e2)\n",
    "stylenet.set_style_layer(3, 'gram', 1e2)\n",
    "stylenet.set_style_layer(4, 'gram', 1e2)\n",
    "# stylenet.set_hist_layer(0, 5e1)\n",
    "# stylenet.set_hist_layer(1, 5e1)\n",
    "# stylenet.set_hist_layer(2, 5e1)\n",
    "# stylenet.set_hist_layer(3, 5e1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# parameters\n",
    "image_size = 1024\n",
    "scale_steps = 3\n",
    "scale_ratio = 2.0\n",
    "style_scale = 1.0\n",
    "\n",
    "\n",
    "content_path = 'examples/inputs/monalisa.jpg'\n",
    "style_paths = ['examples/inputs/starry_night.jpg',\n",
    "               'examples/inputs/cubist.jpg']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "content_image = load_image(content_path, image_size)\n",
    "style_size = int(image_size * style_scale)\n",
    "style_images = [load_image(path, style_size) for path in style_paths]\n",
    "\n",
    "# load content masks for each style image\n",
    "content_mask_paths = ['examples/segments/monalisa1a.png',\n",
    "                      'examples/segments/monalisa1b.png']\n",
    "content_masks = [load_image(path, image_size, to_normalize=False) \n",
    "                 for path in content_mask_paths]\n",
    "\n",
    "\n",
    "\n",
    "aspect = get_aspect_ratio(content_path)\n",
    "image_sizes = [image_size * (scale_ratio ** -s) for s in range(scale_steps)]\n",
    "image_sizes = [[int(size), int(size * aspect)] for size in reversed(image_sizes)]\n",
    "\n",
    "\n",
    "\n",
    "# capture the style and content images\n",
    "stylenet.capture(content_image, style_images, None, content_masks)\n",
    "\n",
    "# initialize input image with a random image\n",
    "# or load it from another image (e.g. content)\n",
    "if True:\n",
    "    img = random_image_like(content_image)\n",
    "elif False:\n",
    "    img = load_image(content_path, image_size)\n",
    "\n",
    "# generate and save\n",
    "num_iterations = 500\n",
    "output_path = 'results/my_example.png'\n",
    "orig_colors = False\n",
    "img = optimize(stylenet, img, num_iterations, output_path, save_preview_iter=10, print_iter=250)\n",
    "if orig_colors:\n",
    "    img = original_colors(content_image, img)\n",
    "save(img, \"results/my_example.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100/400: \n",
      "  Content loss = 2.6e+06\n",
      "  Style loss = 2.3e+03, 3.0e+04, 2.1e+04, 2.5e+05, 1.2e+03\n",
      "  Histogram loss = \n",
      "  TV loss = 3.2e+04\n",
      "  Total loss = 2.95e+06\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# set stylenet hyper-parameters\n",
    "stylenet.set_content_weight(5e0)\n",
    "stylenet.set_style_weight(1e2)\n",
    "stylenet.set_hist_weight(0)\n",
    "stylenet.set_style_statistic('gram')\n",
    "stylenet.set_tv_weight(1e-3)\n",
    "\n",
    "# set stylenet style layers individually\n",
    "stylenet.set_style_layer(0, 'gram', 1e2)\n",
    "stylenet.set_style_layer(1, 'gram', 1e2)\n",
    "stylenet.set_style_layer(2, 'gram', 1e2)\n",
    "stylenet.set_style_layer(3, 'gram', 1e2)\n",
    "stylenet.set_style_layer(4, 'gram', 1e2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# parameters\n",
    "image_size = 1024\n",
    "scale_steps = 3\n",
    "scale_ratio = 2.0\n",
    "style_scale = 1.0\n",
    "num_iterations = 400\n",
    "\n",
    "content_path = 'examples/inputs/monalisa.jpg'\n",
    "style_paths = ['examples/inputs/starry_night.jpg',\n",
    "               'examples/inputs/cubist.jpg']\n",
    "content_mask_paths = ['examples/segments/monalisa1a.png',\n",
    "                      'examples/segments/monalisa1b.png']\n",
    "\n",
    "style_paths = ['examples/inputs/starry_night.jpg']  # make possible to have not list\n",
    "\n",
    "\n",
    "\n",
    "content_image_main = load_image(content_path, image_size)\n",
    "style_size = int(image_size * style_scale)\n",
    "style_images = [load_image(path, style_size) for path in style_paths]\n",
    "\n",
    "# load content masks for each style image\n",
    "content_masks_main = [load_image(path, image_size, to_normalize=False) \n",
    "                      for path in content_mask_paths]\n",
    "\n",
    "aspect = get_aspect_ratio(content_path)\n",
    "image_sizes = [image_size * (scale_ratio ** -s) for s in range(scale_steps)]\n",
    "image_sizes = [[int(size), int(aspect * size)] for size in reversed(image_sizes)]\n",
    "\n",
    "print(image_sizes)\n",
    "h, w = image_sizes[0]\n",
    "print(w, h)\n",
    "img = random_image(h, w)\n",
    "print('img', img.shape)\n",
    "for i, (w, h) in enumerate(image_sizes):\n",
    "    if i > 0:\n",
    "        img = torch.nn.functional.interpolate(img, (w, h), mode='bicubic', align_corners=True)\n",
    "    content_image = torch.nn.functional.interpolate(content_image_main, (w, h), mode='bicubic', align_corners=True)\n",
    "    #content_masks = [torch.nn.functional.interpolate(content_mask, (w, h), mode='bicubic', align_corners=True) for content_mask in content_masks_main]\n",
    "    stylenet.capture(content_image, style_images)#, None, content_masks)\n",
    "    img = optimize(stylenet, img, num_iterations, print_iter=50)\n",
    "    save(img, 'imgh%03d.png' % w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 171)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(image_sizes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Easy style transfer (not implemented yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'size': (768, 1024), 'init': 'random', \n",
    "    'style_paths': ['myimg1.png', 'myimg2.png'],\n",
    "    'style_scale': 2.0,\n",
    "    'num_iterations': [1000, 500, 300, 200],\n",
    "    'octave_scale': 2.0,\n",
    "    'original_colors': False\n",
    "}\n",
    "\n",
    "# make easydict\n",
    "# dictionary of parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512, 343])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (719) : unspecified launch failure at /pytorch/aten/src/THC/THCStorage.cpp:49",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-87bc5eed18e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimg2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m930\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bicubic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimg2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners)\u001b[0m\n\u001b[1;32m   2526\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample_trilinear3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_output_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2527\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bicubic'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2528\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample_bicubic2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_output_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2529\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2530\u001b[0m         raise NotImplementedError(\"Input Error: Only 3D, 4D and 5D input Tensors supported\"\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (719) : unspecified launch failure at /pytorch/aten/src/THC/THCStorage.cpp:49"
     ]
    }
   ],
   "source": [
    "print(img.shape)\n",
    "img2 = torch.nn.functional.interpolate(img, (1024, 930), mode='bicubic', align_corners=True)\n",
    "print(img2.shape) \n",
    "img2.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capturing content targets\n",
      "Capturing style target 1\n",
      "Capturing style target 2\n",
      "Running optimization with L-BFGS\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (686) must match the existing size (687) at non-singleton dimension 3.  Target sizes: [1, 64, 1024, 686].  Tensor sizes: [1, 1024, 687]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-e8acf638abe9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mstylenet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstylenet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'zout3a.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/neural-style-pt/generate.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(stylenet, img, num_iterations, output_path, save_preview_iter, print_iter)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mloopVal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;31m# evaluate initial f(x) and df/dx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0morig_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mcurrent_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/neural-style-pt/generate.py\u001b[0m in \u001b[0;36miterate\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mstylenet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstylenet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/neural-style-pt/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, loss_mode)\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist_losses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'loss'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/neural-style-pt/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmasks\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                     \u001b[0ml_mask_ori\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m                     \u001b[0ml_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml_mask_ori\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m                     \u001b[0ml_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml_mask_ori\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m                     \u001b[0mmasked_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (686) must match the existing size (687) at non-singleton dimension 3.  Target sizes: [1, 64, 1024, 686].  Tensor sizes: [1, 1024, 687]"
     ]
    }
   ],
   "source": [
    "# multiresolution\n",
    "image_size *= 2\n",
    "\n",
    "img = torch.nn.functional.interpolate(img, scale_factor=2.0, mode='bicubic', align_corners=True)\n",
    "content_image = load_image(content_path, image_size)\n",
    "style_images = [load_image(path, int(image_size * style_scale)) for path in style_paths]\n",
    "\n",
    "content_masks = [load_image(path, image_size, to_normalize=False) for path in content_mask_paths]\n",
    "stylenet.capture(content_image, style_images, None, content_masks)\n",
    "\n",
    "img = optimize(stylenet, img, num_iterations, 'zout3a.png', print_iter=250)\n",
    "\n",
    "\n",
    "save(img, 'zout3.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 268, 512])\n",
      "torch.Size([1, 3, 536, 1024])\n",
      "torch.Size([1, 3, 1072, 2048])\n"
     ]
    }
   ],
   "source": [
    "print(img.shape)\n",
    "img2 = torch.nn.functional.interpolate(img, scale_factor=2.0, mode='bicubic', align_corners=True)\n",
    "print(img2.shape)\n",
    "img3 = torch.nn.functional.interpolate(img2, scale_factor=2.0, mode='bicubic', align_corners=True)\n",
    "print(img3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 268, 512])\n",
      "torch.Size([1, 3, 400, 700])\n"
     ]
    }
   ],
   "source": [
    "print(img.shape)\n",
    "img2 = torch.nn.functional.interpolate(img, size=(400, 700), mode='bicubic', align_corners=True)\n",
    "print(img2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 800/800: \n",
      "  Style loss = 8.1e+02, 2.1e+04, 1.2e+04, 1.9e+05, 1.0e+02\n",
      "  TV loss = 1.2e+04\n",
      "  Total loss = 2.37e+05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-43.0551, -42.8480, -33.1289,  ...,  -4.0490,  -4.2271,  -4.2849],\n",
       "          [-44.4197, -42.5732, -31.2063,  ...,  -4.2878,  -4.2909,  -4.3693],\n",
       "          [-44.7566, -43.3370, -21.0557,  ..., -14.9968, -12.7377, -12.7173],\n",
       "          ...,\n",
       "          [-68.6525, -68.6609, -63.4467,  ..., -32.4816,  13.4839,  39.9641],\n",
       "          [-27.9955, -27.9766, -18.9608,  ...,  39.3473,  39.9464,  39.9639],\n",
       "          [-27.9990, -27.9777, -18.9666,  ...,  39.3458,  39.9332,  39.9588]],\n",
       "\n",
       "         [[ 89.2776,  86.4382,  70.7958,  ...,  99.2659,  50.5392,  43.0732],\n",
       "          [ 89.2030,  78.4606,  66.3775,  ...,  55.5223,  50.5102,  -2.5931],\n",
       "          [ 74.6339,  64.5337,  53.3437,  ...,  -2.6036,  -2.6184,  -2.6347],\n",
       "          ...,\n",
       "          [-99.1302, -99.6419, -19.6715,  ..., -37.1758, -29.8861,  38.2951],\n",
       "          [-30.9289, -30.9133, -17.7217,  ...,  -0.7292,  35.6196,  38.3400],\n",
       "          [-30.9274, -30.9182, -17.7269,  ...,  -0.7243,  35.6173,  38.3418]],\n",
       "\n",
       "         [[ 70.0611,  68.6049,  38.7390,  ...,  51.9068,  32.2629,  32.1596],\n",
       "          [ 62.1391,  53.2309,  38.6078,  ...,  29.4118,  29.3503,  -4.5665],\n",
       "          [ 44.1708,  39.1859,  38.4085,  ..., -19.7636, -21.2464, -21.2995],\n",
       "          ...,\n",
       "          [-34.3311, -34.6069,  -7.5402,  ..., -33.2263, -33.0147, -21.7680],\n",
       "          [-35.0548, -35.0560,  -7.4179,  ..., -33.2270, -33.0167, -21.9566],\n",
       "          [-35.1071, -35.1061,  -7.6719,  ..., -33.2292, -33.0214, -32.3655]]]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "        \n",
    "image_size = 720\n",
    "num_iterations = 800\n",
    "output_path = 'out8.png'\n",
    "original_colors = 0\n",
    "style_scale = 1.0\n",
    "print_iter = 100\n",
    "save_iter = 100\n",
    "\n",
    "\n",
    "content_path = 'examples/inputs/hoovertowernight.jpg'\n",
    "style_paths = ['examples/inputs/starry_night.jpg','examples/inputs/cubist.jpg']\n",
    "content_mask_paths = ['examples/segments/hoovertowernight2a.png','examples/segments/hoovertowernight2b.png']\n",
    "\n",
    "# load content image\n",
    "content_image = load_image(content_path, image_size)#.type(dtype)\n",
    "\n",
    "# load style images\n",
    "style_size = int(image_size * style_scale)\n",
    "style_images = [load_image(path, style_size) \n",
    "                for path in style_paths]\n",
    "\n",
    "# load masks\n",
    "content_masks = [load_image(path, image_size, to_normalize=False) \n",
    "                 for path in content_mask_paths]\n",
    "\n",
    "# set hyper-parameters\n",
    "stylenet.set_content_weight(0)\n",
    "stylenet.set_style_weight(1e2)\n",
    "stylenet.set_hist_weight(0)\n",
    "stylenet.set_tv_weight(1e-3)\n",
    "stylenet.set_style_statistic('covariance')\n",
    "# error handling: if capture before, there are problems\n",
    "\n",
    "# capture the style and content images (with masks)\n",
    "stylenet.capture(content_image, style_images, None, content_masks)\n",
    "            \n",
    "\n",
    "# initialize with a random image\n",
    "img = random_image_like(content_image)\n",
    "\n",
    "optimize(stylenet, img, num_iterations, output_path, original_colors, print_iter, save_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 600/600: \n",
      "  Content loss = 8.4e+05\n",
      "  Style loss = 1.7e+05, 3.5e+07, 1.4e+07, 4.7e+08, 1.0e+04\n",
      "  TV loss = 5.7e+04\n",
      "  Total loss = 5.21e+08\n"
     ]
    }
   ],
   "source": [
    "# multi-scaling\n",
    "\n",
    "original_colors = 0\n",
    "style_scale = 1.0\n",
    "print_iter = 100\n",
    "save_iter = 100\n",
    "\n",
    "content_path = 'examples/inputs/monalisa.jpg'\n",
    "style_paths = ['examples/inputs/starry_night.jpg','examples/inputs/cubist.jpg']\n",
    "\n",
    "# load content image\n",
    "content_image = load_image(content_path, image_size)\n",
    "\n",
    "# load style images\n",
    "style_size = int(image_size * style_scale)\n",
    "style_images = [load_image(path, style_size) for path in style_paths]\n",
    "\n",
    "# set hyper-parameters\n",
    "stylenet.set_content_weight(5e0)\n",
    "stylenet.set_style_weight(1e2)\n",
    "stylenet.set_hist_weight(0)\n",
    "stylenet.set_tv_weight(1e-3)\n",
    "stylenet.set_style_statistic('gram')\n",
    "\n",
    "# something like this?\n",
    "#stylenet.set_style_layer(0, 'gram', 1e2)\n",
    "#stylenet.set_style_layer(1, 'covariance', 1e2)\n",
    "\n",
    "# capture the style and content images\n",
    "\n",
    "# initialize with a random image\n",
    "content_image = load_image(content_path, 256)\n",
    "img = random_image_like(content_image)\n",
    "stylenet.capture(content_image, style_images)\n",
    "img = optimize(stylenet, img, 800, 'out1a.png', original_colors, print_iter, save_iter)\n",
    "\n",
    "content_image = load_image(content_path, 512)\n",
    "img = load_image('out1a.png', 512)\n",
    "stylenet.capture(content_image, style_images)\n",
    "img = optimize(stylenet, img, 600, 'out2a.png', original_colors, print_iter, save_iter)\n",
    "\n",
    "content_image = load_image(content_path, 1024)\n",
    "img = load_image('out2a.png', 1024)\n",
    "stylenet.capture(content_image, style_images)\n",
    "img = optimize(stylenet, img, 600, 'out3a.png', original_colors, print_iter, save_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000/1000: \n",
      "  Content loss = 0.0e+00\n",
      "  Style loss = 2.4e+04, 3.1e+06, 1.2e+06, 4.0e+07, 3.0e+03\n",
      "  TV loss = 3.5e+04\n",
      "  Total loss = 4.39e+07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 107.4170,  107.3822,  107.2753,  ...,   18.5964,   18.6580,\n",
       "             18.6587],\n",
       "          [ 107.4086,  107.3838,  107.2558,  ...,   18.5873,   18.6362,\n",
       "             18.6319],\n",
       "          [   6.0468,    6.0569,   26.0255,  ...,   18.5962,   18.6259,\n",
       "             18.6241],\n",
       "          ...,\n",
       "          [ -15.5395,  -15.5464,  -15.5937,  ...,   -4.7783,   -4.8542,\n",
       "             -4.8807],\n",
       "          [ -15.6088,  -15.5838,  -15.5704,  ...,   -4.8293,   -4.8587,\n",
       "             -4.8855],\n",
       "          [ -15.5850,  -15.5833,  -15.5725,  ...,   -4.8222,   -4.8541,\n",
       "             -4.8611]],\n",
       "\n",
       "         [[  20.6844,   20.6578,   20.5571,  ...,   20.3936,   20.4353,\n",
       "             20.4430],\n",
       "          [  20.6818,   20.6415,   20.5252,  ...,   20.3572,   20.4206,\n",
       "             20.4426],\n",
       "          [ -93.5266,  -94.0915,  -97.6999,  ...,   20.3577,   20.4144,\n",
       "             20.4342],\n",
       "          ...,\n",
       "          [ -40.6276,  -40.6149,  -40.6026,  ...,   13.2112,   13.2194,\n",
       "             13.2305],\n",
       "          [ -40.5604,  -40.5715,  -40.5721,  ...,   13.2204,   13.2089,\n",
       "             13.2494],\n",
       "          [ -40.5617,  -40.5551,  -40.5580,  ...,   13.2325,   13.2305,\n",
       "             13.1608]],\n",
       "\n",
       "         [[ -71.0882,  -71.1080,  -71.9495,  ...,   23.5993,   23.8394,\n",
       "             23.8973],\n",
       "          [ -71.1370,  -71.1350,  -71.9137,  ...,   23.5728,   23.8488,\n",
       "             23.8602],\n",
       "          [-116.0931, -113.4171, -113.0039,  ...,   23.5603,   23.8784,\n",
       "             23.9017],\n",
       "          ...,\n",
       "          [ -35.7993,  -35.8265,  -35.8149,  ...,   26.8732,   26.8550,\n",
       "             26.8414],\n",
       "          [ -35.7822,  -35.8037,  -35.8361,  ...,   26.8806,   26.8423,\n",
       "             26.8359],\n",
       "          [ -35.7844,  -35.7783,  -35.8108,  ...,   26.8794,   26.8500,\n",
       "             26.8926]]]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#######################################################3\n",
    "\n",
    "\n",
    "image_size = 1280\n",
    "num_iterations = 1200\n",
    "output_path = 'go5.png'\n",
    "\n",
    "original_colors = 0\n",
    "style_scale = 0.25\n",
    "print_iter = 100\n",
    "save_iter = 100\n",
    "\n",
    "content_path = 'examples/inputs/hoovertowernight.jpg'\n",
    "\n",
    "\n",
    "style_paths = ['examples/inputs/starry_night.jpg','examples/inputs/hokusai.jpg',\n",
    "                'examples/inputs/starry_night.jpg','examples/inputs/hokusai.jpg',\n",
    "                'examples/inputs/starry_night.jpg','examples/inputs/hokusai.jpg',\n",
    "                'examples/inputs/starry_night.jpg','examples/inputs/hokusai.jpg',\n",
    "                'examples/inputs/starry_night.jpg','examples/inputs/hokusai.jpg',\n",
    "                'examples/inputs/starry_night.jpg','examples/inputs/hokusai.jpg']\n",
    "\n",
    "\n",
    "style_blend_weights = [1.0, 0.0,\n",
    "                      0.8, 0.2,\n",
    "                       0.6, 0.4,\n",
    "                       0.4, 0.6,\n",
    "                       0.2, 0.8,                      \n",
    "                      0.0, 1.0]\n",
    "\n",
    "# load content image\n",
    "content_image = load_image(content_path, image_size)\n",
    "\n",
    "# load style images\n",
    "style_size = int(image_size * style_scale)\n",
    "style_images = [load_image(path, style_size) for path in style_paths]\n",
    "\n",
    "stylenet.set_content_weight(0)\n",
    "stylenet.set_style_weight(1e2)\n",
    "stylenet.set_tv_weight(1e-3)\n",
    "\n",
    "# capture the style and content images\n",
    "\n",
    "# initialize with a random image\n",
    "img = random_image_like(content_image)\n",
    "\n",
    "\n",
    "\n",
    "content_masks = [torch.zeros(content_image.shape) for c in range(len(style_images))]\n",
    "m=8\n",
    "content_masks[0][:,:,:,:360+m] = 1.0\n",
    "content_masks[1][:,:,:,:360+m] = 1.0\n",
    "content_masks[2][:,:,:,360-m:500+m] = 1.0\n",
    "content_masks[3][:,:,:,360-m:500+m] = 1.0\n",
    "content_masks[4][:,:,:,500-m:640+m] = 1.0\n",
    "content_masks[5][:,:,:,500-m:640+m] = 1.0\n",
    "content_masks[6][:,:,:,640-m:780+m] = 1.0\n",
    "content_masks[7][:,:,:,640-m:780+m] = 1.0\n",
    "content_masks[8][:,:,:,780-m:920+m] = 1.0\n",
    "content_masks[9][:,:,:,780-m:920+m] = 1.0\n",
    "content_masks[10][:,:,:,920-m:] = 1.0\n",
    "content_masks[11][:,:,:,920-m:] = 1.0\n",
    "\n",
    "\n",
    "stylenet.capture(content_image, style_images, style_blend_weights, content_masks)\n",
    "\n",
    "optimize(stylenet, img, 1000, output_path, original_colors, print_iter, save_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1280) must match the existing size (720) at non-singleton dimension 3.  Target sizes: [1, 3, 673, 1280].  Tensor sizes: [720]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-40ac66a054c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mcontent_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mcontent_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mcontent_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m720\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mcontent_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mcontent_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m720\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (1280) must match the existing size (720) at non-singleton dimension 3.  Target sizes: [1, 3, 673, 1280].  Tensor sizes: [720]"
     ]
    }
   ],
   "source": [
    "import IPython.display\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "from io import BytesIO \n",
    "import IPython.display\n",
    "import numpy as np\n",
    "\n",
    "def showarray(a, fmt='png'):\n",
    "    a = np.uint8(np.array(a))\n",
    "    f = BytesIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
    "\n",
    "x = 0\n",
    "content_masks = [torch.zeros(content_image.shape) for c in range(len(style_images))]\n",
    "content_masks[0][:,:,:,:x] = torch.linspace(0, 1, x)\n",
    "content_masks[0][:,:,:,x:] = torch.linspace(1, 0, 720-x)\n",
    "content_masks[1][:,:,:,:x] = torch.linspace(0, 1, x)\n",
    "content_masks[1][:,:,:,x:] = torch.linspace(1, 0, 720-x)\n",
    "\n",
    "showarray(np.transpose(np.array(255*content_masks[0][0]), [1,2,0]).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#######################################################3\n",
    "\n",
    "\n",
    "image_size = 720\n",
    "num_iterations = 1200\n",
    "output_path = 'go6.png'\n",
    "\n",
    "original_colors = 0\n",
    "style_scale = 1.0\n",
    "print_iter = 100\n",
    "save_iter = 100\n",
    "\n",
    "content_path = 'examples/inputs/hoovertowernight.jpg'\n",
    "style_paths = ['examples/inputs/starry_night.jpg','examples/inputs/hokusai.jpg']\n",
    "\n",
    "# load content image\n",
    "content_image = load_image(content_path, image_size)\n",
    "\n",
    "# load style images\n",
    "style_size = int(image_size * style_scale)\n",
    "style_images = [load_image(path, style_size) for path in style_paths]\n",
    "\n",
    "stylenet.set_content_weight(0)\n",
    "stylenet.set_style_weight(1e2)\n",
    "stylenet.set_tv_weight(1e-3)\n",
    "\n",
    "# capture the style and content images\n",
    "\n",
    "# initialize with a random image\n",
    "img = random_image_like(content_image)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "content_masks = [torch.zeros(content_image.shape) for c in range(len(style_images))]\n",
    "content_masks[0][:,:,:,:] = torch.linspace(0, 1, 720)\n",
    "content_masks[1][:,:,:,:] = torch.linspace(1, 0, 720)\n",
    "#content_masks[2][:,:,:,480:] = 1.0\n",
    "\n",
    "def optimize2(stylenet, img, num_iterations, output_path, original_colors, print_iter=None, save_iter=None):\n",
    "    def iterate():\n",
    "        t[0] += 1\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        stylenet(img)\n",
    "        loss = stylenet.get_loss()\n",
    "        loss.backward()\n",
    "        \n",
    "        maybe_print(stylenet, t[0], print_iter, num_iterations, loss)\n",
    "        maybe_save(img, t[0], save_iter, num_iterations, original_colors, output_path)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    img = nn.Parameter(img.type(stylenet.dtype))\n",
    "    optimizer, loopVal = setup_optimizer(img, stylenet.params, num_iterations)\n",
    "    t = [0]\n",
    "    while t[0] <= loopVal:\n",
    "        optimizer.step(iterate)\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "for z in range(8):\n",
    "    for x in range(0,720,3):\n",
    "        if z%2==0:\n",
    "            content_masks = [torch.zeros(content_image.shape) for c in range(len(style_images))]\n",
    "            content_masks[0][:,:,:,:x] = torch.linspace(0, 1, x)\n",
    "            content_masks[0][:,:,:,x:] = torch.linspace(1, 0, 720-x)\n",
    "            content_masks[1][:,:,:,:x] = torch.linspace(0, 1, x)\n",
    "            content_masks[1][:,:,:,x:] = torch.linspace(1, 0, 720-x)\n",
    "            bw = x/719.0;\n",
    "            style_blend_weights = [bw, 1.0-bw]\n",
    "        else:\n",
    "            content_masks = [torch.zeros(content_image.shape) for c in range(len(style_images))]\n",
    "            content_masks[0][:,:,:,:720-x] = torch.linspace(0, 1, x)\n",
    "            content_masks[0][:,:,:,720-x:] = torch.linspace(1, 0, 720-x)\n",
    "            content_masks[1][:,:,:,:720-x] = torch.linspace(0, 1, x)\n",
    "            content_masks[1][:,:,:,720-x:] = torch.linspace(1, 0, 720-x)\n",
    "            bw = (719.0-x)/719.0;\n",
    "            style_blend_weights = [bw, 1.0-bw]\n",
    "\n",
    "        stylenet.capture(content_image, style_images, None, content_masks)\n",
    "\n",
    "        img = optimize2(stylenet, img, 5, output_path, original_colors, print_iter, save_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimize(stylenet, img, num_iterations, output_path, original_colors, print_iter=None, save_iter=None):\n",
    "    def iterate():\n",
    "        t[0] += 1\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        stylenet(img)\n",
    "        loss = stylenet.get_loss()\n",
    "        loss.backward()\n",
    "        \n",
    "        maybe_print(stylenet, t[0], print_iter, num_iterations, loss)\n",
    "        maybe_save(img, t[0], save_iter, num_iterations, original_colors, output_path)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    img = nn.Parameter(img.type(stylenet.dtype))\n",
    "    optimizer, loopVal = setup_optimizer(img, stylenet.params, num_iterations)\n",
    "    t = [0]\n",
    "    while t[0] <= loopVal:\n",
    "        optimizer.step(iterate)\n",
    "    \n",
    "\n",
    "        \n",
    "image_size = 720\n",
    "num_iterations = 1000\n",
    "output_path = 'try9d.png'\n",
    "original_colors = 0\n",
    "style_scale = 1.0\n",
    "print_iter = 100\n",
    "save_iter = 100\n",
    "\n",
    "\n",
    "content_path = 'examples/inputs/hoovertowernight.jpg'\n",
    "style_paths = ['examples/inputs/starry_night.jpg', 'examples/inputs/hokusai1.jpg']\n",
    "\n",
    "# load content image\n",
    "content_image = load_image(content_path, image_size)#.type(dtype)\n",
    "\n",
    "# load style images\n",
    "style_size = int(image_size * style_scale)\n",
    "style_images = [load_image(path, style_size) \n",
    "                for path in style_paths]\n",
    "\n",
    "#style_images[1] = torch.cat([style_images[0], style_images[2]], axis=2)\n",
    "\n",
    "# load masks\n",
    "content_masks = [torch.zeros(content_image.shape) for c in range(len(style_images))]\n",
    "content_masks[0][:,:,:,:] = torch.linspace(0, 1, 720)\n",
    "content_masks[1][:,:,:,:] = torch.linspace(1, 0, 720)\n",
    "#content_masks[2][:,:,:,480:] = 1.0\n",
    "\n",
    "# capture the style and content images (with masks)\n",
    "stylenet.capture(content_image, style_images, None, content_masks)\n",
    "            \n",
    "# set hyper-parameters\n",
    "stylenet.set_content_weight(0)\n",
    "stylenet.set_style_weight(1e2)\n",
    "stylenet.set_tv_weight(1e-3)\n",
    "\n",
    "# initialize with a random image\n",
    "img = random_image_like(content_image)\n",
    "\n",
    "optimize(stylenet, img, num_iterations, output_path, original_colors, print_iter, save_iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
